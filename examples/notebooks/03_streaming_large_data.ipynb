{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Streaming Large Datasets with pyhdb-rs\n",
    "\n",
    "When working with datasets that don't fit in memory, pyhdb-rs provides\n",
    "**streaming Arrow batches** for efficient processing.\n",
    "\n",
    "## Memory-Efficient Data Pipeline\n",
    "\n",
    "```\n",
    "HANA (100M rows) → Stream batches (64K rows each) → Process → Aggregate\n",
    "                   ↑ constant memory usage!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from pyhdb_rs import connect\n",
    "\n",
    "HANA_URL = os.environ.get(\"HANA_TEST_URI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-reader-section",
   "metadata": {},
   "source": [
    "## RecordBatchReader for Streaming\n",
    "\n",
    "The `execute_arrow_batches()` method returns a PyArrow RecordBatchReader\n",
    "that yields batches on demand, keeping memory usage constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-reader-example",
   "metadata": {},
   "outputs": [],
   "source": "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n    # Get streaming reader (doesn't load all data!)\n    reader = cursor.execute_arrow_batches(\n        \"SELECT * FROM TRANSACTION_HISTORY\",  # Could be billions of rows\n        batch_size=65536,  # 64K rows per batch\n    )\n\n    print(f\"Schema: {reader.schema}\")\n\n    # Process batches one at a time\n    total_rows = 0\n    for batch in reader:\n        total_rows += batch.num_rows\n        # Process batch...\n\n    print(f\"Processed {total_rows:,} rows\")"
  },
  {
   "cell_type": "markdown",
   "id": "streaming-aggregation-section",
   "metadata": {},
   "source": [
    "## Streaming Aggregation\n",
    "\n",
    "Compute aggregates over huge datasets without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-aggregation",
   "metadata": {},
   "outputs": [],
   "source": "def streaming_aggregate(cursor, query: str, agg_column: str) -> dict:\n    \"\"\"\n    Compute sum, count, min, max over a streaming result set.\n    Memory usage: O(1) regardless of data size.\n    \"\"\"\n    reader = cursor.execute_arrow_batches(query, batch_size=65536)\n\n    total_sum = 0.0\n    total_count = 0\n    min_val = float(\"inf\")\n    max_val = float(\"-inf\")\n\n    for batch in reader:\n        # Convert to Polars for fast aggregation\n        df = pl.from_arrow(batch)\n        col = df[agg_column]\n\n        # Update running aggregates\n        total_sum += col.sum()\n        total_count += len(col)\n        min_val = min(min_val, col.min())\n        max_val = max(max_val, col.max())\n\n    return {\n        \"sum\": total_sum,\n        \"count\": total_count,\n        \"mean\": total_sum / total_count if total_count > 0 else 0,\n        \"min\": min_val,\n        \"max\": max_val,\n    }\n\n\n# Example usage\nwith connect(HANA_URL) as conn, conn.cursor() as cursor:\n    stats = streaming_aggregate(\n        cursor, \"SELECT NET_AMOUNT FROM TRANSACTIONS WHERE FISCAL_YEAR = 2026\", \"NET_AMOUNT\"\n    )\n\nprint(\"Statistics:\")\nfor key, value in stats.items():\n    print(f\"  {key}: {value:,.2f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "grouped-streaming-section",
   "metadata": {},
   "source": [
    "## Grouped Streaming Aggregation\n",
    "\n",
    "When you need group-by aggregates over large data, accumulate partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouped-streaming",
   "metadata": {},
   "outputs": [],
   "source": "from collections import defaultdict\n\n\ndef streaming_group_by(cursor, query: str, group_col: str, agg_col: str) -> pl.DataFrame:\n    \"\"\"\n    Streaming group-by aggregation.\n    Memory usage: O(groups) not O(rows).\n    \"\"\"\n    reader = cursor.execute_arrow_batches(query, batch_size=65536)\n\n    # Accumulators per group\n    group_sums = defaultdict(float)\n    group_counts = defaultdict(int)\n\n    for batch in reader:\n        df = pl.from_arrow(batch)\n\n        # Aggregate this batch\n        batch_agg = df.group_by(group_col).agg(\n            [\n                pl.col(agg_col).sum().alias(\"sum\"),\n                pl.len().alias(\"count\"),\n            ]\n        )\n\n        # Merge into global accumulators\n        for row in batch_agg.iter_rows(named=True):\n            key = row[group_col]\n            group_sums[key] += row[\"sum\"]\n            group_counts[key] += row[\"count\"]\n\n    # Build final result\n    return pl.DataFrame(\n        {\n            group_col: list(group_sums.keys()),\n            \"sum\": list(group_sums.values()),\n            \"count\": list(group_counts.values()),\n            \"mean\": [s / c for s, c in zip(group_sums.values(), group_counts.values())],\n        }\n    ).sort(\"sum\", descending=True)\n\n\n# Example: Revenue by region over 100M transactions\nwith connect(HANA_URL) as conn, conn.cursor() as cursor:\n    result = streaming_group_by(\n        cursor, \"SELECT SALES_REGION, NET_AMOUNT FROM TRANSACTIONS\", \"SALES_REGION\", \"NET_AMOUNT\"\n    )\n\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "id": "chunked-export-section",
   "metadata": {},
   "source": [
    "## Chunked Export to Parquet\n",
    "\n",
    "Export large query results directly to Parquet files without loading into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunked-export",
   "metadata": {},
   "outputs": [],
   "source": "import pyarrow.parquet as pq\n\n\ndef export_to_parquet(cursor, query: str, output_path: str, batch_size: int = 65536):\n    \"\"\"\n    Stream query results directly to Parquet file.\n    Memory usage: O(batch_size) not O(total_rows).\n    \"\"\"\n    reader = cursor.execute_arrow_batches(query, batch_size=batch_size)\n\n    writer = None\n    total_rows = 0\n\n    try:\n        for batch in reader:\n            if writer is None:\n                # Create writer with schema from first batch\n                writer = pq.ParquetWriter(output_path, batch.schema, compression=\"snappy\")\n\n            # Write batch to parquet\n            table = pa.Table.from_batches([batch])\n            writer.write_table(table)\n            total_rows += batch.num_rows\n\n            # Progress indicator\n            print(f\"\\rExported {total_rows:,} rows...\", end=\"\", flush=True)\n    finally:\n        if writer:\n            writer.close()\n\n    print(f\"\\nCompleted: {total_rows:,} rows → {output_path}\")\n    return total_rows\n\n\n# Export example\nwith connect(HANA_URL) as conn, conn.cursor() as cursor:\n    export_to_parquet(cursor, \"SELECT * FROM SALES_ITEMS WHERE FISCAL_YEAR = 2026\", \"sales_2026.parquet\")"
  },
  {
   "cell_type": "markdown",
   "id": "parallel-section",
   "metadata": {},
   "source": [
    "## Parallel Processing with Multiple Connections\n",
    "\n",
    "For maximum throughput, use multiple connections to process partitioned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-processing",
   "metadata": {},
   "outputs": [],
   "source": "from concurrent.futures import ThreadPoolExecutor, as_completed\n\n\ndef process_partition(partition_id: int, total_partitions: int) -> dict:\n    \"\"\"\n    Process one partition of data.\n    Each thread gets its own connection.\n    \"\"\"\n    with connect(HANA_URL) as conn, conn.cursor() as cursor:\n        # Use modulo partitioning\n        df = pl.from_arrow(cursor.execute_arrow(f\"\"\"\n            SELECT SALES_REGION, SUM(NET_AMOUNT) as TOTAL\n            FROM TRANSACTIONS\n            WHERE MOD(TRANSACTION_ID, {total_partitions}) = {partition_id}\n            GROUP BY SALES_REGION\n        \"\"\"))\n\n        return {\n            \"partition\": partition_id,\n            \"data\": df,\n        }\n\n\n# Process 8 partitions in parallel\nNUM_PARTITIONS = 8\nresults = []\nstart = time.time()\n\nwith ThreadPoolExecutor(max_workers=NUM_PARTITIONS) as executor:\n    futures = {\n        executor.submit(process_partition, i, NUM_PARTITIONS): i for i in range(NUM_PARTITIONS)\n    }\n\n    for future in as_completed(futures):\n        result = future.result()\n        results.append(result[\"data\"])\n        print(f\"Partition {result['partition']} complete\")\n\n# Combine results\ncombined = pl.concat(results).group_by(\"SALES_REGION\").agg(pl.col(\"TOTAL\").sum())\nelapsed = time.time() - start\n\nprint(f\"\\nCompleted in {elapsed:.2f}s\")\nprint(combined)"
  },
  {
   "cell_type": "markdown",
   "id": "incremental-section",
   "metadata": {},
   "source": [
    "## Incremental Processing Pattern\n",
    "\n",
    "Process data incrementally using watermarks for exactly-once semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incremental-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json",
    "from pathlib import Path",
    "",
    "",
    "class IncrementalProcessor:",
    "    \"\"\"",
    "    Process data incrementally with checkpointing.",
    "    \"\"\"",
    "",
    "    def __init__(self, checkpoint_file: str):",
    "        self.checkpoint_file = Path(checkpoint_file)",
    "        self.watermark = self._load_watermark()",
    "",
    "    def _load_watermark(self) -> str:",
    "        if self.checkpoint_file.exists():",
    "            data = json.loads(self.checkpoint_file.read_text())",
    "            return data.get(\"watermark\", \"1970-01-01 00:00:00\")",
    "        return \"1970-01-01 00:00:00\"",
    "",
    "    def _save_watermark(self, watermark: str):",
    "        self.checkpoint_file.write_text(json.dumps({\"watermark\": watermark}))",
    "",
    "    def process_new_data(self, cursor) -> pl.DataFrame:",
    "        \"\"\"",
    "        Fetch and process only new data since last run.",
    "        \"\"\"",
    "        print(f\"Processing data since: {self.watermark}\")",
    "",
    "        df = pl.from_arrow(cursor.execute_arrow(f\"\"\"",
    "            SELECT *",
    "            FROM EVENTS",
    "            WHERE EVENT_TIME > '{self.watermark}'",
    "            ORDER BY EVENT_TIME",
    "        \"\"\"))",
    "",
    "        if len(df) > 0:",
    "            # Update watermark to latest event",
    "            new_watermark = df[\"EVENT_TIME\"].max()",
    "            self._save_watermark(str(new_watermark))",
    "            self.watermark = str(new_watermark)",
    "            print(f\"Processed {len(df)} new events, watermark: {self.watermark}\")",
    "        else:",
    "            print(\"No new data\")",
    "",
    "        return df",
    "",
    "",
    "# Example usage",
    "processor = IncrementalProcessor(\"checkpoint.json\")",
    "",
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:",
    "    new_data = processor.process_new_data(cursor)",
    "    if len(new_data) > 0:",
    "        # Process new data...",
    "        print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-monitoring-section",
   "metadata": {},
   "source": [
    "## Memory Monitoring\n",
    "\n",
    "Monitor memory usage during streaming operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-monitoring",
   "metadata": {},
   "outputs": [],
   "source": "import tracemalloc\n\n\ndef measure_streaming_memory(cursor, query: str, batch_size: int = 65536):\n    \"\"\"\n    Measure memory usage during streaming.\n    \"\"\"\n    tracemalloc.start()\n\n    reader = cursor.execute_arrow_batches(query, batch_size=batch_size)\n\n    peak_memory = 0\n    total_rows = 0\n\n    for batch in reader:\n        total_rows += batch.num_rows\n\n        # Check current memory\n        current, peak = tracemalloc.get_traced_memory()\n        peak_memory = max(peak_memory, peak)\n\n    tracemalloc.stop()\n\n    return {\n        \"total_rows\": total_rows,\n        \"peak_memory_mb\": peak_memory / 1024 / 1024,\n        \"bytes_per_row\": peak_memory / total_rows if total_rows > 0 else 0,\n    }\n\n\n# Compare memory usage\nwith connect(HANA_URL) as conn, conn.cursor() as cursor:\n    stats = measure_streaming_memory(cursor, \"SELECT * FROM TRANSACTION_HISTORY\")\n\nprint(f\"Processed {stats['total_rows']:,} rows\")\nprint(f\"Peak memory: {stats['peak_memory_mb']:.2f} MB\")\nprint(f\"Memory per row: {stats['bytes_per_row']:.2f} bytes\")"
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "## Best Practices for Large Data\n",
    "\n",
    "1. **Use streaming** (`execute_arrow_batches`) for datasets > 1M rows\n",
    "2. **Push filters to HANA** - let the database do the heavy lifting\n",
    "3. **Tune batch size** - 64K rows is a good default, adjust based on row width\n",
    "4. **Use Polars lazy mode** for complex transformations\n",
    "5. **Partition queries** for parallel processing\n",
    "6. **Export directly to Parquet** for long-term storage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}