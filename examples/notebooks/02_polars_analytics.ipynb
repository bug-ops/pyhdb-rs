{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Advanced Polars Analytics with pyhdb-rs\n",
    "\n",
    "This notebook demonstrates how to leverage the **zero-copy Arrow integration**\n",
    "between SAP HANA and Polars for high-performance analytics.\n",
    "\n",
    "## Why Zero-Copy Matters\n",
    "\n",
    "Traditional workflow:\n",
    "```\n",
    "HANA \u2192 Network \u2192 Python objects \u2192 pandas/numpy \u2192 Analysis\n",
    "       \u2191 slow        \u2191 GC pressure      \u2191 memory copy\n",
    "```\n",
    "\n",
    "pyhdb-rs workflow:\n",
    "```\n",
    "HANA \u2192 Network \u2192 Arrow buffers \u2192 Polars\n",
    "                 \u2191 zero-copy, no Python objects!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import polars as pl\n",
    "from pyhdb_rs import connect\n",
    "\n",
    "HANA_URL = os.environ.get(\"HANA_TEST_URI\")\n",
    "\n",
    "# Polars configuration for large datasets\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_fmt_str_lengths(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pycapsule-section",
   "metadata": {},
   "source": [
    "## Arrow PyCapsule Interface Integration\n",
    "\n",
    "Behind the scenes, Polars uses the `__arrow_c_stream__` protocol for zero-copy integration.\n",
    "This protocol is part of the Arrow PyCapsule Interface and enables seamless data transfer\n",
    "without Python object creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pycapsule-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "    # execute_arrow() returns a RecordBatchReader\n",
    "    reader = cursor.execute_arrow(\"SELECT * FROM SALES WHERE SALE_DATE >= '2024-01-01'\")\n",
    "\n",
    "    # Polars automatically uses __arrow_c_stream__ protocol\n",
    "    df = pl.from_arrow(reader)\n",
    "\n",
    "    print(f\"Loaded {len(df):,} rows via zero-copy transfer\")\n",
    "    print(f\"Memory: {df.estimated_size() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pycapsule-note",
   "metadata": {},
   "source": [
    "Using Arrow integration with Polars does this automatically, but using",
    "`execute_arrow()` + `pl.from_arrow()` gives you access to the intermediate",
    "`RecordBatchReader` if you need to inspect metadata or process batches manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-section",
   "metadata": {},
   "source": [
    "## LazyFrame for Deferred Execution\n",
    "\n",
    "Polars LazyFrames allow query optimization before execution.\n",
    "Combined with HANA's query pushdown, you get optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_data() -> pl.LazyFrame:",
    "    \"\"\"Load sales data as LazyFrame for deferred processing.\"\"\"",
    "    with connect(HANA_URL) as conn, conn.cursor() as cursor:",
    "        # Push filtering to HANA - only transfer what we need",
    "        df = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "                SELECT ",
    "                    SALE_ID,",
    "                    SALE_DATE,",
    "                    CUSTOMER_ID,",
    "                    PRODUCT_ID,",
    "                    QUANTITY,",
    "                    UNIT_PRICE,",
    "                    DISCOUNT,",
    "                    REGION",
    "                FROM SALES",
    "                WHERE SALE_DATE >= '2024-01-01'",
    "            \"\"\"))",
    "        return df.lazy()",
    "",
    "",
    "# Create LazyFrame - no computation yet!",
    "sales_lf = load_sales_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (still lazy - no execution)\n",
    "result = (\n",
    "    sales_lf.with_columns(\n",
    "        [\n",
    "            # Calculate total amount\n",
    "            (pl.col(\"QUANTITY\") * pl.col(\"UNIT_PRICE\") * (1 - pl.col(\"DISCOUNT\"))).alias(\"TOTAL\"),\n",
    "            # Extract month\n",
    "            pl.col(\"SALE_DATE\").dt.month().alias(\"MONTH\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"TOTAL\") > 100)  # Filter calculated column\n",
    "    .group_by([\"REGION\", \"MONTH\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"TOTAL\").sum().alias(\"REVENUE\"),\n",
    "            pl.col(\"SALE_ID\").count().alias(\"ORDER_COUNT\"),\n",
    "            pl.col(\"TOTAL\").mean().alias(\"AVG_ORDER_VALUE\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"REGION\", \"MONTH\"])\n",
    ")\n",
    "\n",
    "# Now execute and collect results\n",
    "monthly_revenue = result.collect()\n",
    "print(monthly_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window-section",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Polars provides powerful window functions for running totals, rankings, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:",
    "    df = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "            SELECT ",
    "                EMPLOYEE_ID,",
    "                DEPARTMENT,",
    "                SALE_DATE,",
    "                AMOUNT",
    "            FROM EMPLOYEE_SALES",
    "            WHERE SALE_DATE >= '2024-01-01'",
    "        \"\"\"))",
    "",
    "# Window functions in Polars",
    "result = df.with_columns(",
    "    [",
    "        # Running total per employee",
    "        pl.col(\"AMOUNT\").cum_sum().over(\"EMPLOYEE_ID\").alias(\"RUNNING_TOTAL\"),",
    "        # Rank within department",
    "        pl.col(\"AMOUNT\").rank(descending=True).over(\"DEPARTMENT\").alias(\"DEPT_RANK\"),",
    "        # Percentage of department total",
    "        (pl.col(\"AMOUNT\") / pl.col(\"AMOUNT\").sum().over(\"DEPARTMENT\") * 100)",
    "        .round(2)",
    "        .alias(\"DEPT_PERCENTAGE\"),",
    "        # Moving average (7-day)",
    "        pl.col(\"AMOUNT\").rolling_mean(window_size=7).over(\"EMPLOYEE_ID\").alias(\"MA_7D\"),",
    "    ]",
    ")",
    "",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "join-section",
   "metadata": {},
   "source": [
    "## Efficient Joins\n",
    "\n",
    "Load dimension tables once, join in Polars for repeated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:",
    "    # Load fact table (large)",
    "    orders = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "            SELECT ORDER_ID, CUSTOMER_ID, PRODUCT_ID, QUANTITY, ORDER_DATE",
    "            FROM ORDERS ",
    "            WHERE ORDER_DATE >= '2024-01-01'",
    "        \"\"\"))",
    "",
    "    # Load dimension tables (small, can cache)",
    "    customers = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "            SELECT CUSTOMER_ID, CUSTOMER_NAME, SEGMENT, COUNTRY",
    "            FROM CUSTOMERS",
    "        \"\"\"))",
    "",
    "    products = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "            SELECT PRODUCT_ID, PRODUCT_NAME, CATEGORY, UNIT_PRICE",
    "            FROM PRODUCTS",
    "        \"\"\"))",
    "",
    "print(f\"Orders: {len(orders):,} rows\")",
    "print(f\"Customers: {len(customers):,} rows\")",
    "print(f\"Products: {len(products):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and analyze in Polars (very fast!)\n",
    "enriched = (\n",
    "    orders.join(customers, on=\"CUSTOMER_ID\", how=\"left\")\n",
    "    .join(products, on=\"PRODUCT_ID\", how=\"left\")\n",
    "    .with_columns((pl.col(\"QUANTITY\") * pl.col(\"UNIT_PRICE\")).alias(\"TOTAL\"))\n",
    ")\n",
    "\n",
    "# Segment analysis\n",
    "segment_analysis = (\n",
    "    enriched.group_by([\"SEGMENT\", \"CATEGORY\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"TOTAL\").sum().alias(\"REVENUE\"),\n",
    "            pl.col(\"ORDER_ID\").n_unique().alias(\"ORDERS\"),\n",
    "            pl.col(\"CUSTOMER_ID\").n_unique().alias(\"CUSTOMERS\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns((pl.col(\"REVENUE\") / pl.col(\"ORDERS\")).round(2).alias(\"AVG_ORDER_VALUE\"))\n",
    "    .sort(\"REVENUE\", descending=True)\n",
    ")\n",
    "\n",
    "print(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pivot-section",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "Create pivot tables for cross-tabulation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue by region (pivot)\n",
    "monthly_pivot = (\n",
    "    enriched.with_columns(pl.col(\"ORDER_DATE\").dt.strftime(\"%Y-%m\").alias(\"MONTH\"))\n",
    "    .group_by([\"COUNTRY\", \"MONTH\"])\n",
    "    .agg(pl.col(\"TOTAL\").sum().alias(\"REVENUE\"))\n",
    "    .pivot(\n",
    "        on=\"MONTH\",\n",
    "        index=\"COUNTRY\",\n",
    "        values=\"REVENUE\",\n",
    "    )\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "print(monthly_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-section",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "Polars has excellent support for time series operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeseries-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:",
    "    # Load time series data",
    "    metrics = pl.from_arrow(cursor.execute_arrow(\"\"\"",
    "            SELECT ",
    "                TIMESTAMP,",
    "                SENSOR_ID,",
    "                TEMPERATURE,",
    "                HUMIDITY,",
    "                PRESSURE",
    "            FROM IOT_METRICS",
    "            WHERE TIMESTAMP >= ADD_DAYS(CURRENT_TIMESTAMP, -30))",
    "            ORDER BY TIMESTAMP",
    "        \"\"\")",
    "",
    "# Resample to hourly aggregates",
    "hourly = (",
    "    metrics.sort(\"TIMESTAMP\")",
    "    .group_by_dynamic(",
    "        \"TIMESTAMP\",",
    "        every=\"1h\",",
    "        group_by=\"SENSOR_ID\",",
    "    )",
    "    .agg(",
    "        [",
    "            pl.col(\"TEMPERATURE\").mean().alias(\"AVG_TEMP\"),",
    "            pl.col(\"TEMPERATURE\").min().alias(\"MIN_TEMP\"),",
    "            pl.col(\"TEMPERATURE\").max().alias(\"MAX_TEMP\"),",
    "            pl.col(\"HUMIDITY\").mean().alias(\"AVG_HUMIDITY\"),",
    "            pl.len().alias(\"READINGS\"),",
    "        ]",
    "    )",
    ")",
    "",
    "print(f\"Resampled to {len(hourly):,} hourly records\")",
    "print(hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using rolling statistics\n",
    "anomalies = (\n",
    "    hourly.sort([\"SENSOR_ID\", \"TIMESTAMP\"])\n",
    "    .with_columns(\n",
    "        [\n",
    "            # Rolling mean and std\n",
    "            pl.col(\"AVG_TEMP\").rolling_mean(window_size=24).over(\"SENSOR_ID\").alias(\"ROLLING_MEAN\"),\n",
    "            pl.col(\"AVG_TEMP\").rolling_std(window_size=24).over(\"SENSOR_ID\").alias(\"ROLLING_STD\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        # Z-score for anomaly detection\n",
    "        ((pl.col(\"AVG_TEMP\") - pl.col(\"ROLLING_MEAN\")) / pl.col(\"ROLLING_STD\"))\n",
    "        .abs()\n",
    "        .alias(\"Z_SCORE\")\n",
    "    )\n",
    "    .filter(pl.col(\"Z_SCORE\") > 3)  # More than 3 standard deviations\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(anomalies)} anomalies\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Polars supports various output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet (columnar, compressed)\n",
    "segment_analysis.write_parquet(\"segment_analysis.parquet\")\n",
    "\n",
    "# Export to CSV\n",
    "segment_analysis.write_csv(\"segment_analysis.csv\")\n",
    "\n",
    "# Export to JSON\n",
    "segment_analysis.write_json(\"segment_analysis.json\")\n",
    "\n",
    "# Convert to pandas for visualization libraries\n",
    "pandas_df = segment_analysis.to_pandas()\n",
    "\n",
    "print(\"Exported to parquet, csv, and json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-section",
   "metadata": {},
   "source": [
    "## Memory Efficiency\n",
    "\n",
    "Check memory usage of your DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_mb(df: pl.DataFrame) -> float:\n",
    "    \"\"\"Calculate DataFrame memory usage in MB.\"\"\"\n",
    "    return df.estimated_size() / 1024 / 1024\n",
    "\n",
    "\n",
    "print(f\"Orders: {memory_usage_mb(orders):.2f} MB\")\n",
    "print(f\"Enriched: {memory_usage_mb(enriched):.2f} MB\")\n",
    "print(f\"Segment Analysis: {memory_usage_mb(segment_analysis):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}