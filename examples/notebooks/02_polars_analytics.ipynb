{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Advanced Polars Analytics with pyhdb-rs\n",
    "\n",
    "This notebook demonstrates how to leverage the **zero-copy Arrow integration**\n",
    "between SAP HANA and Polars for high-performance analytics.\n",
    "\n",
    "## Why Zero-Copy Matters\n",
    "\n",
    "Traditional workflow:\n",
    "```\n",
    "HANA → Network → Python objects → pandas/numpy → Analysis\n",
    "       ↑ slow        ↑ GC pressure      ↑ memory copy\n",
    "```\n",
    "\n",
    "pyhdb-rs workflow:\n",
    "```\n",
    "HANA → Network → Arrow buffers → Polars\n",
    "                 ↑ zero-copy, no Python objects!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import polars as pl\n",
    "from pyhdb_rs import connect\n",
    "\n",
    "HANA_URL = os.environ.get(\"HANA_TEST_URI\")\n",
    "\n",
    "# Polars configuration for large datasets\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_fmt_str_lengths(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pycapsule-section",
   "metadata": {},
   "source": [
    "## Arrow PyCapsule Interface Integration\n",
    "\n",
    "Behind the scenes, Polars uses the `__arrow_c_stream__` protocol for zero-copy integration.\n",
    "This protocol is part of the Arrow PyCapsule Interface and enables seamless data transfer\n",
    "without Python object creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pycapsule-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "    # execute_arrow() returns a RecordBatchReader\n",
    "    reader = cursor.execute_arrow(\"SELECT * FROM SALES_ITEMS WHERE SALE_DATE >= '2026-01-01'\")\n",
    "\n",
    "    # Polars automatically uses __arrow_c_stream__ protocol\n",
    "    df = pl.from_arrow(reader)\n",
    "\n",
    "    print(f\"Loaded {len(df):,} rows via zero-copy transfer\")\n",
    "    print(f\"Memory: {df.estimated_size() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pycapsule-note",
   "metadata": {},
   "source": [
    "Using Arrow integration with Polars does this automatically, but using\n",
    "`execute_arrow()` + `pl.from_arrow()` gives you access to the intermediate\n",
    "`RecordBatchReader` if you need to inspect metadata or process batches manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-section",
   "metadata": {},
   "source": [
    "## LazyFrame for Deferred Execution\n",
    "\n",
    "Polars LazyFrames allow query optimization before execution.\n",
    "Combined with HANA's query pushdown, you get optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_data() -> pl.LazyFrame:\n",
    "    \"\"\"Load sales data as LazyFrame for deferred processing.\"\"\"\n",
    "    with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "        # Push filtering to HANA - only transfer what we need\n",
    "        df = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "            SELECT\n",
    "                SALE_ID,\n",
    "                SALE_DATE,\n",
    "                CUSTOMER_ID,\n",
    "                PRODUCT_ID,\n",
    "                QUANTITY,\n",
    "                UNIT_PRICE,\n",
    "                DISCOUNT_RATE,\n",
    "                SALES_REGION\n",
    "            FROM SALES_ITEMS\n",
    "            WHERE SALE_DATE >= '2026-01-01'\n",
    "        \"\"\"))\n",
    "        return df.lazy()\n",
    "\n",
    "# Create LazyFrame - no computation yet!\n",
    "sales_lf = load_sales_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy-transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (still lazy - no execution)\n",
    "result = (\n",
    "    sales_lf.with_columns(\n",
    "        [\n",
    "            # Calculate total amount\n",
    "            (pl.col(\"QUANTITY\") * pl.col(\"UNIT_PRICE\") * (1 - pl.col(\"DISCOUNT_RATE\"))).alias(\"NET_AMOUNT\"),\n",
    "            # Extract month\n",
    "            pl.col(\"SALE_DATE\").dt.month().alias(\"MONTH\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"NET_AMOUNT\") > 100)  # Filter calculated column\n",
    "    .group_by([\"SALES_REGION\", \"MONTH\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"NET_AMOUNT\").sum().alias(\"REVENUE\"),\n",
    "            pl.col(\"SALE_ID\").count().alias(\"ORDER_COUNT\"),\n",
    "            pl.col(\"NET_AMOUNT\").mean().alias(\"AVG_ORDER_VALUE\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"SALES_REGION\", \"MONTH\"])\n",
    ")\n",
    "\n",
    "# Now execute and collect results\n",
    "monthly_revenue = result.collect()\n",
    "print(monthly_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window-section",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Polars provides powerful window functions for running totals, rankings, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "    df = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "        SELECT\n",
    "            EMPLOYEE_ID,\n",
    "            DEPARTMENT,\n",
    "            SALE_DATE,\n",
    "            NET_AMOUNT\n",
    "        FROM EMPLOYEE_SALES\n",
    "        WHERE SALE_DATE >= '2026-01-01'\n",
    "    \"\"\"))\n",
    "\n",
    "# Window functions in Polars\n",
    "result = df.with_columns(\n",
    "    [\n",
    "        # Running total per employee\n",
    "        pl.col(\"NET_AMOUNT\").cum_sum().over(\"EMPLOYEE_ID\").alias(\"RUNNING_TOTAL\"),\n",
    "        # Rank within department\n",
    "        pl.col(\"NET_AMOUNT\").rank(descending=True).over(\"DEPARTMENT\").alias(\"DEPT_RANK\"),\n",
    "        # Percentage of department total\n",
    "        (pl.col(\"NET_AMOUNT\") / pl.col(\"NET_AMOUNT\").sum().over(\"DEPARTMENT\") * 100)\n",
    "        .round(2)\n",
    "        .alias(\"DEPT_PERCENTAGE\"),\n",
    "        # Moving average (7-day)\n",
    "        pl.col(\"NET_AMOUNT\").rolling_mean(window_size=7).over(\"EMPLOYEE_ID\").alias(\"MA_7D\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "join-section",
   "metadata": {},
   "source": [
    "## Efficient Joins\n",
    "\n",
    "Load dimension tables once, join in Polars for repeated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "    # Load fact table (large)\n",
    "    orders = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "        SELECT ORDER_ID, CUSTOMER_ID, PRODUCT_ID, QUANTITY, ORDER_DATE\n",
    "        FROM SALES_ORDERS\n",
    "        WHERE ORDER_DATE >= '2026-01-01'\n",
    "    \"\"\"))\n",
    "\n",
    "    # Load dimension tables (small, can cache)\n",
    "    customers = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "        SELECT CUSTOMER_ID, CUSTOMER_NAME, CUSTOMER_SEGMENT, COUNTRY\n",
    "        FROM CUSTOMERS\n",
    "    \"\"\"))\n",
    "\n",
    "    products = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "        SELECT PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY, NET_PRICE\n",
    "        FROM PRODUCTS\n",
    "    \"\"\"))\n",
    "\n",
    "print(f\"Orders: {len(orders):,} rows\")\n",
    "print(f\"Customers: {len(customers):,} rows\")\n",
    "print(f\"Products: {len(products):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and analyze in Polars (very fast!)\n",
    "enriched = (\n",
    "    orders.join(customers, on=\"CUSTOMER_ID\", how=\"left\")\n",
    "    .join(products, on=\"PRODUCT_ID\", how=\"left\")\n",
    "    .with_columns((pl.col(\"QUANTITY\") * pl.col(\"NET_PRICE\")).alias(\"NET_AMOUNT\"))\n",
    ")\n",
    "\n",
    "# Segment analysis\n",
    "segment_analysis = (\n",
    "    enriched.group_by([\"CUSTOMER_SEGMENT\", \"PRODUCT_CATEGORY\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"NET_AMOUNT\").sum().alias(\"REVENUE\"),\n",
    "            pl.col(\"ORDER_ID\").n_unique().alias(\"ORDERS\"),\n",
    "            pl.col(\"CUSTOMER_ID\").n_unique().alias(\"CUSTOMERS\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns((pl.col(\"REVENUE\") / pl.col(\"ORDERS\")).round(2).alias(\"AVG_ORDER_VALUE\"))\n",
    "    .sort(\"REVENUE\", descending=True)\n",
    ")\n",
    "\n",
    "print(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pivot-section",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "Create pivot tables for cross-tabulation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue by region (pivot)\n",
    "monthly_pivot = (\n",
    "    enriched.with_columns(pl.col(\"ORDER_DATE\").dt.strftime(\"%Y-%m\").alias(\"MONTH\"))\n",
    "    .group_by([\"COUNTRY\", \"MONTH\"])\n",
    "    .agg(pl.col(\"NET_AMOUNT\").sum().alias(\"REVENUE\"))\n",
    "    .pivot(\n",
    "        on=\"MONTH\",\n",
    "        index=\"COUNTRY\",\n",
    "        values=\"REVENUE\",\n",
    "    )\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "print(monthly_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-section",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "Polars has excellent support for time series operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeseries-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(HANA_URL) as conn, conn.cursor() as cursor:\n",
    "    # Load time series data\n",
    "    metrics = pl.from_arrow(cursor.execute_arrow(\"\"\"\n",
    "        SELECT\n",
    "            TIMESTAMP,\n",
    "            SENSOR_ID,\n",
    "            TEMPERATURE,\n",
    "            HUMIDITY,\n",
    "            PRESSURE\n",
    "        FROM IOT_METRICS\n",
    "        WHERE TIMESTAMP >= ADD_DAYS(CURRENT_TIMESTAMP, -30)\n",
    "        ORDER BY TIMESTAMP\n",
    "    \"\"\"))\n",
    "\n",
    "# Resample to hourly aggregates\n",
    "hourly = (\n",
    "    metrics.sort(\"TIMESTAMP\")\n",
    "    .group_by_dynamic(\n",
    "        \"TIMESTAMP\",\n",
    "        every=\"1h\",\n",
    "        group_by=\"SENSOR_ID\",\n",
    "    )\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"TEMPERATURE\").mean().alias(\"AVG_TEMP\"),\n",
    "            pl.col(\"TEMPERATURE\").min().alias(\"MIN_TEMP\"),\n",
    "            pl.col(\"TEMPERATURE\").max().alias(\"MAX_TEMP\"),\n",
    "            pl.col(\"HUMIDITY\").mean().alias(\"AVG_HUMIDITY\"),\n",
    "            pl.len().alias(\"READINGS\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Resampled to {len(hourly):,} hourly records\")\n",
    "print(hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using rolling statistics\n",
    "anomalies = (\n",
    "    hourly.sort([\"SENSOR_ID\", \"TIMESTAMP\"])\n",
    "    .with_columns(\n",
    "        [\n",
    "            # Rolling mean and std\n",
    "            pl.col(\"AVG_TEMP\").rolling_mean(window_size=24).over(\"SENSOR_ID\").alias(\"ROLLING_MEAN\"),\n",
    "            pl.col(\"AVG_TEMP\").rolling_std(window_size=24).over(\"SENSOR_ID\").alias(\"ROLLING_STD\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        # Z-score for anomaly detection\n",
    "        ((pl.col(\"AVG_TEMP\") - pl.col(\"ROLLING_MEAN\")) / pl.col(\"ROLLING_STD\"))\n",
    "        .abs()\n",
    "        .alias(\"Z_SCORE\")\n",
    "    )\n",
    "    .filter(pl.col(\"Z_SCORE\") > 3)  # More than 3 standard deviations\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(anomalies)} anomalies\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Polars supports various output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet (columnar, compressed)\n",
    "segment_analysis.write_parquet(\"segment_analysis.parquet\")\n",
    "\n",
    "# Export to CSV\n",
    "segment_analysis.write_csv(\"segment_analysis.csv\")\n",
    "\n",
    "# Export to JSON\n",
    "segment_analysis.write_json(\"segment_analysis.json\")\n",
    "\n",
    "# Convert to pandas for visualization libraries\n",
    "pandas_df = segment_analysis.to_pandas()\n",
    "\n",
    "print(\"Exported to parquet, csv, and json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-section",
   "metadata": {},
   "source": [
    "## Memory Efficiency\n",
    "\n",
    "Check memory usage of your DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_mb(df: pl.DataFrame) -> float:\n",
    "    \"\"\"Calculate DataFrame memory usage in MB.\"\"\"\n",
    "    return df.estimated_size() / 1024 / 1024\n",
    "\n",
    "\n",
    "print(f\"Orders: {memory_usage_mb(orders):.2f} MB\")\n",
    "print(f\"Enriched: {memory_usage_mb(enriched):.2f} MB\")\n",
    "print(f\"Segment Analysis: {memory_usage_mb(segment_analysis):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhdb-rs (.venv)",
   "language": "python",
   "name": "pyhdb-rs"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
